# TD: data analysis with pandas and matplotlib

## pandas and pyplot introduction with reaction times
We will first use a script that presents a series of trials in which a white dot is presented at the center of the screen and the participant must click on the mouse as quickly as possible. The script measures the reaction times and records them in a csv file.

If you are a good programmer, try to write this script. Otherwise, we will focus on the data analysis. Run the script `simple-detection-visual-pygame.py` contained in folder `reaction-times` to acquire the data. Check that the data has been acquired correctly by typing `cat  reaction_times.csv` in the terminal.

We will do some exploratory analysis. You can use ipython in your terminal, or the ipython console in spyder (lower right corner). We first have to import pandas and open the data file:
```py
import pandas as pd
df = pd.read_csv('reaction_times.csv')
```

The variable `df` is now a pandas dataframe, storing all the data read from the csv. You should now be able to look at its shape, and even read the whole data since it is quite small:
```py
print(df.shape)
print(df)
```
Note that the column names are automatically inferred from the first line of the csv.

There are many ways to access data elements in pandas, but here are the main ones:
```py
df['RT']  # basic indexer: access columns by their names
df.RT   # attribute indexer: for column names (if they don't contain spaces)
df.loc[0, 'RT']  # loc indexer: elements by row and column names
df.iloc[0, 1]  # iloc indexer: elements by position, like in numpy
df.RT[0]  # another way... Multiple combinations are actually possible
```

For all these, slices can be used, for example:
```py
df.loc[1:3, 'RT']
```

Also note a very useful feature: the possibility to index via a test (called boolean indexing):
```py
df[df.RT > 400]
```

Try to plot some statistics:
```py
print(df.RT.mean())
print(df.RT.std()) # standard deviation
print(df.RT.median())
print(df.RT.count())
print(df.RT.max())
print(df.RT.min())
```

The objective is then to combine these to plot new interesting features. Try the following exercices:
* compute the percentage of trials having a reaction time superior to 400ms
* Find the first quartile of the reaction times, ie the smallest reaction times such that 25% of reaction times are smaller. (First use the method `df.RT.sort_values()`. Then infer manually the position of the first quartile among sorted values)
* (more challenging) print the reaction times that are more than 1 standard deviations away from the mean. (have a look at boolean operators for indexing https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#boolean-indexing)

Next, import pyplot to do some figures, like a histogram:
```py
import matplotlib.pyplot as plt
plt.hist(df.RT)
plt.xlabel('reaction time')
plt.ylabel('count')
```
(on spyder, use ctrl+enter to not print immediately the result but write another line)

or a bar plot:
```py
plt.bar(df.Trial, df.RT)
plt.xlabel('trial number')
plt.ylabel('reaction time')
```

For the curious, you might also want to look at the package seaborn for some easy to do aesthetically appealing plots:
```py
import seaborn as sns
sns.distplot(df.RT, rug=True)
```

## A more complex dataset: the titanic dataset
The website kaggle.com which hosts machine learning competitions has a famous dataset for beginners, the titanic dataset, in which the aim is predict the chances of survival of passengers of the titanic based on many variables such as their age, gender, or the price of their ticket.

### Exercice 1
1) Open the dataset with pandas. Two very useful functions to get a first look at a dataset are `df.columns` which will give you the column names and `df.describe()` which will give you some summary statistics for each column. Look at the different columns and try to understand what type of data they contain. How many total passengers are there?

2) Plot the distribution of the ages of the passengers as a histogram. Same for the fares.

3) How many passengers survived?

### Exercice 2 : categorical data
1) Columns like Survived contain data of a finite number of categories (in this case 0 or 1). A very useful method for handling such data is `value_counts()`. Try `df.Survived.value_counts()`. Apply it to compute the number of passengers of each sex, the number of passengers by class, and the number of embarked passengers.

2) Can you compute the total proportion of passengers who survived? Notice that `df.Survived.sum()` gives you the number of passengers who survived. Can you the method `mean()` to also get your result?

3) If you are unsure about a column, the method `nunique()` gives you the number of distinct values in a column. The column `SibSp` contains the number of siblings of each passenger. What is the distribution of this data? What about the columns `Name` and `Ticket`?

### Exercice 3 : missing data
With the `count` method, count the number of values in `Age` and `Cabin`. How do they compare with other columns? This is due to missing data, registered as `NaN` in the dataframe. Look at the first few values of these columns to see what we are talking about. Check the extent of missing data in the whole dataset with `df.isna().sum()`

### Exercice 4 : looking for correlations
At the core of data analysis is the idea of comparing how variables correlate to each other, and in particular what differences might exist in the distribution of some variables among different groups.

1) One first variable that should clearly correlate with survival is sex. Compute the proportion of passengers that survived for each sex, using the methods that you know. Do you notice a difference? (For the statistically minded students: how would you go about quantifying the statistical significance of this result?)

2) One can generalize this type of analysis with the concept of aggregate values and pivot tables. In a pivot table, you take one categorical variable (say the sex), and you want to compute how another variable (survival) depends on the first, by computing an aggregate (for example sum, mean, or number of distinct values are aggregate) of the second variable for each value of the first variable. The function `pivot_table` enables you to do this analysis in a straightforward manner. Try the following:
```py
df.pivot_table(index=['Sex'], values=['Survived'], aggfunc='mean')
```
This gives you exacty what you want! (note: some of you may know this as the feature "tableau crois√© dynamique" from Excel). Also notice that the arguments index and values actually take list, so you could create a pivot table with more than 2 variables.
I'll give another solution for the sake of completeness, which involves the function `groupby`. Try this:
```py
df.groupby('Sex').Survived.mean()
```
You see this often, however the `groupby` function creates objects that can be hard to manipulate. I advise you to use it if `pivot_table` does not give enough flexibility only.

1) Do the same type of analysis to see if Pclass has an effect on survival.

2) Know we would like to see if the price of the ticket correlates with survival. What would you expect? Can you compute the mean ticket price for people who survived and those who didn't? (do it in one line!) Do you notice a difference?

3) Since the variable Fare here is continuous, we might want to plot its entire distribution for the survived and non-survived groups. One way to do this is with multi-group histograms. Try:
 ```py
 plt.hist([df.Fare[df.Survived == 0], df.Fare[df.Survived == 1]])
 ```
 Notice that the function `plt.hist` can take a list of data to plot histograms next to each other. Do you see a difference? Add the argument `density=True` to the hist function to normalize each group by its size. Is the effect clearer now?
   
6) How do you expect the Age to correlate with survival? Compute the mean age for those who survived and those who didn't. Do you get a huge difference? Now plot the normalized histograms. What do you see? Can you explain why the means were not so different despite important differences in the distributions?

7) The final case of correlations that one might want to look are correlations between two continous variables. Do you expect fare to correlate with age? Let us look at a cloud of points with:
```py
plt.scatter(df.Age, df.Fare)
```

### Bonus: Exercice 5 : correlations between continuous variables and basic linear regression
The cloud of points does not seem conclusive, but a correlation cannot be ruled out. Let us try to fit a linear regression to the data. Try this code:
```py
from scipy import stats
import numpy
slope, intercept, r, p, _ = stats.linregress(df.Age[~df.Age.isnull()], df.Fare[~df.Age.isnull()])
print('R^2: {:.3f}'.format(r**2))
print('p-value: {:.2f}'.format(p))
plt.scatter(df.Age, df.Fare)
xs = np.linspace(0, 100, 100)
plt.plot(xs, slope * xs + intercept, c='red')
```
A few things about the code: since the stats package does not know about missing values from pandas, these have to be removed first, with the boolean index `~df.Age.isnull()` (since only Age contained missing values, and not Fare). `linregress` gives with you the linear model, along with values `r` (whose square is the variance explained, often called R^2), and a p-value for statistical significance of non-zero slope (read the documentation here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html). Notice here that the R^2 is very small (a linear model does not explain the data), but that the p-value is also small (which means there is a statistically significant correlation). Finally, to plot the linear fit, we have to compute it for a series of x values. Usually, the function `np.linspace(a, b, n)` comes in very handy for this: it creates a series of n equally spaced values between a and b. We can then apply the linear function to the whole series as if it was a single number with `slope * xs + intercept`. 

This is a basic example of using the package `scipy.stats`. More complete statistical models, similar to what can be found in R, are in the `statsmodels` package (https://www.statsmodels.org/stable/index.html). Try to do the same thing with this package. Notice that you get other scores, for example p-values for the intercept, and that you can do multivariate linear regression. If you want to learn more about linear regression in python: https://365datascience.com/linear-regression/

## Solutions to the reaction times small exercices
```py
# 1
total = df.RT.count()
bigger = df.RT[df.RT > 400].count()
print(bigger / total * 100)

# 2
quartile_position = math.ceil(total * 0.25)  # or do it manually
print(df.RT.sort_values().iloc[quartile_position])

# 3
# Solution 1
df.RT[(df.RT - mu > sigma) | (df.RT - mu < -sigma)]
# Solution 2
df.RT[(df.RT - mu > sigma).abs()]
```


